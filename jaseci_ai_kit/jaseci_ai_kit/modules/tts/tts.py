import os
import sys
import time
import json
import torch
import base64
import urllib.request

import numpy as np

from fastapi import HTTPException
from scipy.io.wavfile import write
from jaseci.actions.live_actions import jaseci_action
from jaseci.actions.remote_actions import launch_server

from .tacotron2.model import Tacotron2
from .waveglow.model import WaveGlow
from speechbrain.pretrained import Tacotron2 as SpeechBrain
from speechbrain.pretrained import HIFIGAN as HIFIGAN


taco_checkpoint = "https://api.ngc.nvidia.com/v2/models/nvidia/tacotron2_pyt_ckpt_amp/versions/19.09.0/files/nvidia_tacotron2pyt_fp16_20190427"
waglo_checkpoint = "https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_amp/versions/19.09.0/files/nvidia_waveglowpyt_fp16_20190427"

force_reload = False
rate = 22050


def _download_checkpoint(checkpoint, force_reload):
    """
    This method download the tacotron checkpoint weights from the checkpoint url

    Parameters:
    -----------
    checkpoint: String, url to the checkpoint.
    force_reload: Boolean.

    Return:
    -----------
    ckpt_file: String, Path to downloaded checkpoint file

    """
    model_dir = os.path.join(torch.hub._get_torch_home(), "checkpoints")
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    ckpt_file = os.path.join(model_dir, os.path.basename(checkpoint))
    if not os.path.exists(ckpt_file) or force_reload:
        sys.stderr.write("Downloading checkpoint from {}\n".format(checkpoint))
        urllib.request.urlretrieve(checkpoint, ckpt_file)
    return ckpt_file


def _checkpoint_from_distributed(state_dict):
    """
    Checks whether checkpoint was generated by DistributedDataParallel. DDP
    wraps model in additional "module.", it needs to be unwrapped for single
    GPU inference.

    Parameters:
    -----------
    state_dict: Dictionary

    Return:
    -----------
    ret: Boolean

    """
    ret = False
    for key, _ in state_dict.items():
        if key.find("module.") != -1:
            ret = True
            break
    return ret


def _unwrap_distributed(state_dict):
    """
    Unwraps model from DistributedDataParallel.
    DDP wraps model in additional "module.", it needs to be removed for single
    GPU inference.

    Parameters:
    -----------

    state_dict: Dictionary, model's state dict

    Return:
    -----------

    new_state_dict = Dictionary, modified model's state dict
    """
    new_state_dict = {}
    for key, value in state_dict.items():
        new_key = key.replace("module.1.", "")
        new_key = new_key.replace("module.", "")
        new_state_dict[new_key] = value
    return new_state_dict


def load_tacotron(checkpoint, force_reload):
    """
    Downloads tacotron2 checkpoints and loading state dictionery

    Parameters:
    -----------
    checkpoint: String, url to the tacotron2 checkpoint.
    force_reload: Boolean. setting this value to true will ignore the downloaded checkpoints from the cache.
    Return:
    -----------
    tacotron2: Model Tacotron
    """
    ckpt_file = _download_checkpoint(checkpoint, force_reload)
    ckpt = torch.load(ckpt_file, map_location=torch.device("cpu"))
    state_dict = ckpt["state_dict"]
    if _checkpoint_from_distributed(state_dict):
        state_dict = _unwrap_distributed(state_dict)
    config = ckpt["config"]
    tacotron2 = Tacotron2(**config)
    tacotron2.load_state_dict(state_dict)

    return tacotron2


def load_waveglow(checkpoint, force_reload):
    """
    Downloads waveglow checkpoints and loading state dictionery

    Parameters:
    -----------
    checkpoint: String, url to the waveglow checkpoint.
    force_reload: Boolean. setting this value to true will ignore the downloaded checkpoints from the cache.
    Return:
    -----------
    waveglow: Model, waveglow
    """

    ckpt_file = _download_checkpoint(checkpoint, force_reload)
    ckpt = torch.load(ckpt_file, map_location=torch.device("cpu"))
    state_dict = ckpt["state_dict"]
    if _checkpoint_from_distributed(state_dict):
        state_dict = _unwrap_distributed(state_dict)
    config = ckpt["config"]
    waveglow = WaveGlow(**config)
    waveglow.load_state_dict(state_dict)

    return waveglow


def make_utils():
    """
    Downloading neccessary utils from torch hub and loading tacotron2 and waveglow models.

    Return:
    -----------
    tacotron2: Model, tacotron2
    waveglow: Model, waveglow
    utils: Processing
    """
    tacotron2 = load_tacotron(taco_checkpoint, force_reload)
    waveglow = load_waveglow(waglo_checkpoint, force_reload)
    waveglow = waveglow.remove_weightnorm(waveglow)
    utils = torch.hub.load("NVIDIA/DeepLearningExamples:torchhub", "nvidia_tts_utils")

    return tacotron2, waveglow, utils


def _pretrained_models_file_path(model_name):
    """
    Creating a space in cache to save the downloaded model checkpoint.
    Parameters:
    -----------
    model_name: String, name of the pretrained model.

    Return:
    -----------
    model_ckpt_file: Str, name of the checkpoint directory.
    """
    model_dir = os.path.join(torch.hub._get_torch_home(), "pretrained_models")
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    model_ckpt_file = os.path.join(model_dir, os.path.basename(model_name))
    return model_ckpt_file


# loading tacotron from nvidia and waveglow model from nvidia, preprocessing utils from nvidia
tacotron2, waveglow, utils = make_utils()
# loading tacotron from speechbrain
speech_brain = SpeechBrain.from_hparams(
    source="speechbrain/tts-tacotron2-ljspeech",
    savedir=_pretrained_models_file_path("speechbrain_taco"),
)
# loading hifigan vocorder
hifi_gan = HIFIGAN.from_hparams(
    source="speechbrain/tts-hifigan-libritts-22050Hz",
    savedir=_pretrained_models_file_path("hifi_gan"),
)


def prediction(input_text, seq2seqmodel=tacotron2, vocorder="hifi_gan", utils=utils):
    """
    Inferencing

    Parameters:
    -----------
    input_text: String, input text for preprocessing.
    seq2seqmodel: Model, the sequence to sequence model.
    vocorder: Model, the vocorder model.
    utils: Processing,

    Return:
    -----------
    audio_numpy: Numpy array, 1d numpy with floats contains audio data.
    """
    if seq2seqmodel == "tacotron2":
        seq2seqmodel = tacotron2
    elif seq2seqmodel == "speechbrain":
        seq2seqmodel = speech_brain
    else:
        print("Print no valid vocorder")

    sequences, lengths = utils.prepare_input_sequence([input_text], cpu_run=True)

    with torch.no_grad():
        mel, _, _ = seq2seqmodel.infer(sequences, lengths)
        if vocorder == "waveglow":
            audio = waveglow.infer(mel)
            audio_numpy = audio[0].data.cpu().numpy()
        elif vocorder == "hifi_gan":
            audio = hifi_gan.decode_batch(mel)
            audio_numpy = audio[0].data.cpu().numpy()[0]
        else:
            print("no valid vocorder")
    return audio_numpy


def save_file(input_numpy, path="", rate=rate):
    """
    Saving the audio file the given path

    Parameters:
    -----------
    input_numpy: Numpy array, 1d numpy with floats contains audio data.
    path: String, path to the directory to save the file.
    rate: int, The rate of the audio.

    Return:
    -----------
    success: Boolean, True if successfuly saved.
    file_path: String, path to the saved file.
    """
    success = False
    ret_dict = {
        "save_status": success,
    }
    if os.path.exists(path):
        try:
            file_name = "audio_file_" + str(time.time()) + ".wav"
            file_path = os.path.join(path, file_name)
            write(file_path, rate, input_numpy)
            success = True
            ret_dict = {
                "save_status": success,
                "file_path": file_path,
            }
        except Exception as ex:
            print(ex)
            success = False
            file_path = None
    else:
        print("Set up directory path properly to save the audio file.")

    return ret_dict


@jaseci_action(act_group=["tts"], allow_remote=True)
def synthesize(
    text: str,
    seq2seq_model: str = "tacotron2",
    vocorder: str = "waveglow",
    base64_val: bool = False,
    path: str = "",
    rate: int = rate,
):
    try:
        synthesize_audio = prediction(text, seq2seq_model, vocorder)
        if base64_val:
            json_encoded_list = json.dumps(synthesize_audio.tolist())
            output_list = base64.b64encode(json_encoded_list)
        else:
            output_list = synthesize_audio.tolist()

        if path != "":
            audio_data = np.array(output_list, dtype="float32")
            status = save_file(audio_data, path, rate)
            ret = {"audio_wave": output_list, "saving_status": status}
        else:
            output_list = synthesize_audio.tolist()
            ret = {
                "audio_wave": output_list,
            }
        return ret
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@jaseci_action(act_group=["tts"], allow_remote=True)
def save_audio(audio_data: list, path: str = "", rate: int = rate):
    try:
        audio_data = np.array(audio_data, dtype="float32")
        status = save_file(audio_data, path, rate)
        return status
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    print("Text to Speech Synthesizer up and running")
    save_file(
        prediction(
            "This is a test run",
            seq2seqmodel=speech_brain,
            vocorder="hifi_gan",
            utils=utils,
        ),
        "./",
    )
    # launch_server(port=8000)
