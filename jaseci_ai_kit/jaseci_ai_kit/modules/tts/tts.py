import os
import sys
import time
import torch
import urllib.request

from fastapi import HTTPException
from scipy.io.wavfile import write
from jaseci.actions.live_actions import jaseci_action
from jaseci.actions.remote_actions import launch_server

from .tacotron2.model import Tacotron2
from waveglow import model as Waveglow

taco_checkpoint = "https://api.ngc.nvidia.com/v2/models/nvidia/tacotron2_pyt_ckpt_amp/versions/19.09.0/files/nvidia_tacotron2pyt_fp16_20190427"
waglo_checkpoint = "https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_amp/versions/19.09.0/files/nvidia_waveglowpyt_fp16_20190427"

force_reload = True


def _download_checkpoint(checkpoint, force_reload):
    """
    This method downloada the tacotron checkpoint weights from the checkpoint url

    Parameters:
    -----------
    checkpoint: String, url to the checkpoint.
    force_reload: Boolean.

    Return:
    -----------
    ckpt_file: String, Path to downloaded checkpoint file

    """
    model_dir = os.path.join(torch.hub._get_torch_home(), "checkpoints")
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    ckpt_file = os.path.join(model_dir, os.path.basename(checkpoint))
    if not os.path.exists(ckpt_file) or force_reload:
        sys.stderr.write("Downloading checkpoint from {}\n".format(checkpoint))
        urllib.request.urlretrieve(checkpoint, ckpt_file)
    return ckpt_file


def _checkpoint_from_distributed(state_dict):
    """
    Checks whether checkpoint was generated by DistributedDataParallel. DDP
    wraps model in additional "module.", it needs to be unwrapped for single
    GPU inference.

    Parameters:
    -----------
    state_dict: Dictionary

    Return:
    -----------
    ret: Boolean

    """
    ret = False
    for key, _ in state_dict.items():
        if key.find("module.") != -1:
            ret = True
            break
    return ret


def _unwrap_distributed(state_dict):
    """
    Unwraps model from DistributedDataParallel.
    DDP wraps model in additional "module.", it needs to be removed for single
    GPU inference.

    Parameters:
    -----------

    state_dict: Dictionary, model's state dict

    Return:
    -----------

    new_state_dict = Dictionary, modified model's state dict
    """
    new_state_dict = {}
    for key, value in state_dict.items():
        new_key = key.replace("module.1.", "")
        new_key = new_key.replace("module.", "")
        new_state_dict[new_key] = value
    return new_state_dict


def load_tacotron(checkpoint, force_reload):
    """
    Download tacotron2 checkpoints and loading state dictionery

    Parameters:
    -----------
    checkpoint: checkpoint file

    Return:
    -----------
    tacotron2 = Model
    """
    ckpt_file = _download_checkpoint(checkpoint, force_reload)
    ckpt = torch.load(ckpt_file, map_location=torch.device("cpu"))
    state_dict = ckpt["state_dict"]
    if _checkpoint_from_distributed(state_dict):
        state_dict = _unwrap_distributed(state_dict)
    config = ckpt["config"]
    tacotron2 = Tacotron2.Tacotron2(**config)
    tacotron2.load_state_dict(state_dict)

    return tacotron2


def load_waveglow(checkpoint, force_reload):

    ckpt_file = _download_checkpoint(checkpoint, force_reload)
    ckpt = torch.load(ckpt_file, map_location=torch.device("cpu"))
    state_dict = ckpt["state_dict"]
    if _checkpoint_from_distributed(state_dict):
        state_dict = _unwrap_distributed(state_dict)
    config = ckpt["config"]
    waveglow = Waveglow.WaveGlow(**config)
    waveglow.load_state_dict(state_dict)

    return waveglow


def make_utils(force_reload):

    tacotron2 = load_tacotron(taco_checkpoint, force_reload)
    waveglow = load_waveglow(waglo_checkpoint, force_reload)
    waveglow = waveglow.remove_weightnorm(waveglow)
    utils = torch.hub.load("NVIDIA/DeepLearningExamples:torchhub", "nvidia_tts_utils")

    return tacotron2, waveglow, utils


tacotron2, waveglow, utils = make_utils()


def prediction(input_text, model=tacotron2, vocorder=waveglow, utils=utils):

    sequences, lengths = utils.prepare_input_sequence([input_text], cpu_run=True)
    with torch.no_grad():
        mel, _, _ = tacotron2.infer(sequences, lengths)
        audio = waveglow.infer(mel)
    audio_numpy = audio[0].data.cpu().numpy()
    rate = 22050

    write("audio.wav", rate, audio_numpy)

    return audio_numpy


@jaseci_action(act_group=["synthesize"], allow_remote=True)
def synthesize(text: str, threshold: float = 0.7):
    try:
        synthesize_audio = prediction(input_text=text)
        return synthesize_audio
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":

    print("Text Segmentor up and running")
    launch_server(port=8000)
