<!DOCTYPE html><html><head><title>MTLLM API Documentation | Language Models</title><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"><meta name="robots" content="index,follow"><meta name="theme-color" content="#212121"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><link href="https://fonts.googleapis.com/css?family=Hind:400,700&amp;display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:300,400&amp;display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/icon?family=Material+Icons%7CMaterial+Icons+Outlined&amp;display=swap" rel="stylesheet"><style>
      body, input, button {
        font-family: 'Hind', sans-serif;
      }

      code, .hljs {
        font-family: 'Source Code Pro', 'Courier New', Courier, monospace;
      }

      .icon-font {
        font-family: 'Material Icons';
        font-weight: normal;
        font-style: normal;
        font-size: 24px;  /* Preferred icon size */
        display: inline-block;
        line-height: 1;
        text-transform: none;
        letter-spacing: normal;
        word-wrap: normal;
        white-space: nowrap;
        direction: ltr;

        /* Support for all WebKit browsers. */
        -webkit-font-smoothing: antialiased;
        /* Support for Safari and Chrome. */
        text-rendering: optimizeLegibility;

        /* Support for Firefox. */
        -moz-osx-font-smoothing: grayscale;

        /* Support for IE. */
        font-feature-settings: 'liga';
      }

      .icon-font.outline {
        font-family: 'Material Icons Outlined';
      }
    </style><script src="https://cdn.jsdelivr.net/npm/mermaid@8.11.2/dist/mermaid.min.js"></script><script>mermaid.initialize({"startOnLoad":true});
                        window.addEventListener('navigation', () => { mermaid.init(); });
                        console.log('Mermaid Plugin Initialized');</script><link href="/mtllm/docs/assets/codedoc-styles.css" rel="stylesheet"><script async="" defer="" src="/mtllm/docs/assets/codedoc-bundle.js"></script></head><body><div class="header-0-0-9"><script async="" defer="" src="https://buttons.github.io/buttons.js"></script><a class="github-button" data-color-scheme="no-preference: light; light: light; dark: dark;" data-icon="false" data-show-count="true" data-size="false" href="https://github.com/Jaseci-Labs/mtllm/">Star</a><br><br></div><div id="-codedoc-container" class="container"><h1 id="language-models" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>Language Models</h1><p>Language models is the most important building block of MTLLM. Without it we can't achieve neuro-symbolic programming.</p><p>Let's first make sure you can set up your language model. MTLLM support clients for many remote and local LMs. You can even create your own as well very easily if you want to.</p><h2 id="setting-up-a-lm-client" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>Setting up a LM client</h2><p>In this section, we will go through the process of setting up a OpenAI's <code>GPT-4o</code> language model client. For that first makesure that you have installed the necessary dependancies by running <code>pip install mtllm[openai]</code>.</p><pre class="with-bar"><code class="python -codedoc-code-snippet code-0-0-2" tabindex="0"><span class="wmbar-0-0-7"><span></span><span></span><span></span><span></span></span><div class="line-0-0-6  -codedoc-code-line" data-content="import from mtllm.llms.openai, OpenAI;" id="code1-l1"><span class="lineCounter-0-0-3 prim -codedoc-line-counter">1<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span><span class="token keyword">import</span><span class="token punctuation">:</span>py <span class="token keyword">from</span> mtllm<span class="token punctuation">.</span>llms<span class="token punctuation">.</span>openai<span class="token punctuation">,</span> OpenAI<span class="token punctuation">;</span></div><br><div class="line-0-0-6  -codedoc-code-line" data-content="" id="code1-l2"><span class="lineCounter-0-0-3 -codedoc-line-counter">2<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span></div><br><div class="line-0-0-6  -codedoc-code-line" data-content="my_llm = OpenAI(model_name=&quot;gpt-4o&quot;);" id="code1-l3"><span class="lineCounter-0-0-3 prim -codedoc-line-counter">3<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span>my_llm <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>model_name<span class="token operator">=</span><span class="token string">"gpt-4o"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></div><br></code></pre><p>Makesure to set the <code>OPENAI_API_KEY</code> environment variable with your OpenAI API key.</p><h2 id="directly-calling-the-lm" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>Directly calling the LM</h2><p>You can directly call the LM by giving the raw prompts as well.</p><pre class=""><code class="python -codedoc-code-snippet code-0-0-2" tabindex="0"><span class="wmbar-0-0-7"><span></span><span></span><span></span><span></span></span><div class="line-0-0-6  -codedoc-code-line" data-content="my_llm(&quot;What is the capital of France?&quot;);" id="code2-l1"><span class="lineCounter-0-0-3 prim -codedoc-line-counter">1<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span>my_llm<span class="token punctuation">(</span><span class="token string">"What is the capital of France?"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></div><br></code></pre><p>You can also pass the <code>max_tokens</code>, <code>temperature</code> and other parameters to the LM.</p><pre class=""><code class="python -codedoc-code-snippet code-0-0-2" tabindex="0"><span class="wmbar-0-0-7"><span></span><span></span><span></span><span></span></span><div class="line-0-0-6  -codedoc-code-line" data-content="my_llm(&quot;What is the capital of France?&quot;, max_tokens=10, temperature=0.5);" id="code3-l1"><span class="lineCounter-0-0-3 prim -codedoc-line-counter">1<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span>my_llm<span class="token punctuation">(</span><span class="token string">"What is the capital of France?"</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">;</span></div><br></code></pre><h2 id="using-the-lm-with-mtllm" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>Using the LM with MTLLM</h2><p>Intented use of MTLLM's LMs is to use them with the <code>jaclang</code>'s <code>BY_LLM</code> Feature.</p><h3 id="with-abilities-and-methods" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>With Abilities and Methods</h3><pre class=""><code class="python -codedoc-code-snippet code-0-0-2" tabindex="0"><span class="wmbar-0-0-7"><span></span><span></span><span></span><span></span></span><div class="line-0-0-6  -codedoc-code-line" data-content="can function(arg1: str, arg2: str) -> str by llm();" id="code4-l1"><span class="lineCounter-0-0-3 prim -codedoc-line-counter">1<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span>can function<span class="token punctuation">(</span>arg1<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> arg2<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">str</span> by llm<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></div><br></code></pre><h3 id="with-classes" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>With Classes</h3><pre class=""><code class="python -codedoc-code-snippet code-0-0-2" tabindex="0"><span class="wmbar-0-0-7"><span></span><span></span><span></span><span></span></span><div class="line-0-0-6  -codedoc-code-line" data-content="new_object = MyClass(arg1: str by llm());" id="code5-l1"><span class="lineCounter-0-0-3 prim -codedoc-line-counter">1<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span>new_object <span class="token operator">=</span> MyClass<span class="token punctuation">(</span>arg1<span class="token punctuation">:</span> <span class="token builtin">str</span> by llm<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span></div><br></code></pre><h3 id="you-can-parse-following-attributes-to-the-by-llm-feature" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>You can parse following attributes to the <code>by llm()</code> feature:</h3><ul><li><code>method</code> (default: <code>Normal</code>): Reasoning method to use. Can be <code>Normal</code>, <code>Reason</code> or <code>Chain-of-Thoughts</code>.</li><li><code>tools</code> (default: <code>None</code>): Tools to use. This is a list of abilities to use with ReAct Prompting method.</li><li><code>model specific parameters</code>: You can pass the model specific parameters as well. for example, <code>max_tokens</code>, <code>temperature</code> etc.</li></ul><h2 id="enabling-verbose-mode" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>Enabling Verbose Mode</h2><p>You can enable the verbose mode to see the internal workings of the LM.</p><pre class="with-bar"><code class="python -codedoc-code-snippet code-0-0-2" tabindex="0"><span class="wmbar-0-0-7"><span></span><span></span><span></span><span></span></span><div class="line-0-0-6  -codedoc-code-line" data-content="import from mtllm.llms, OpenAI;" id="code6-l1"><span class="lineCounter-0-0-3 prim -codedoc-line-counter">1<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span><span class="token keyword">import</span><span class="token punctuation">:</span>py <span class="token keyword">from</span> mtllm<span class="token punctuation">.</span>llms<span class="token punctuation">,</span> OpenAI<span class="token punctuation">;</span></div><br><div class="line-0-0-6  -codedoc-code-line" data-content="" id="code6-l2"><span class="lineCounter-0-0-3 -codedoc-line-counter">2<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span></div><br><div class="line-0-0-6  -codedoc-code-line" data-content="my_llm = OpenAI(model_name=&quot;gpt-4o&quot;, verbose=True);" id="code6-l3"><span class="lineCounter-0-0-3 prim -codedoc-line-counter">3<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span>my_llm <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>model_name<span class="token operator">=</span><span class="token string">"gpt-4o"</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">;</span></div><br></code></pre><h2 id="remote-lms" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>Remote LMs</h2><p>These language models are provided as managed services. To access them, simply sign up and obtain an API key. Before calling any of the remote language models listed below.</p><blockquote><p><strong>NOTICE</strong></p><p>make sure to set the corresponding environment variable with your API key. Use Chat models for better performance.</p></blockquote><pre class=""><code class="python -codedoc-code-snippet code-0-0-2" tabindex="0"><span class="wmbar-0-0-7"><span></span><span></span><span></span><span></span></span><div class="line-0-0-6  -codedoc-code-line" data-content="llm = mtllm.llms.{provider_listed_below}(model_name=&quot;your model&quot;, verbose=True/False);" id="code7-l1"><span class="lineCounter-0-0-3 prim -codedoc-line-counter">1<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span>llm <span class="token operator">=</span> mtllm<span class="token punctuation">.</span>llms<span class="token punctuation">.</span><span class="token punctuation">{</span>provider_listed_below<span class="token punctuation">}</span><span class="token punctuation">(</span>model_name<span class="token operator">=</span><span class="token string">"your model"</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token operator">/</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">;</span></div><br></code></pre><ol><li><code>OpenAI</code> - OpenAI's gpt-3.5-turbo, gpt-4, gpt-4-turbo, gpt-4o <a href="https://platform.openai.com/docs/models">model zoo</a></li><li><code>Anthropic</code> - Anthropic's Claude 3 &amp; Claude 3.5 - Haiku ,Sonnet, Opus <a href="https://docs.anthropic.com/en/docs/about-claude/models">model zoo</a></li><li><code>Groq</code> - Groq's Fast Inference Models <a href="https://console.groq.com/docs/models">model zoo</a></li><li><code>Together</code> - Together's hosted OpenSource Models <a href="https://docs.together.ai/docs/inference-models">model zoo</a></li></ol><h2 id="local-lms" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>Local LMs</h2><h3 id="ollama" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>Ollama</h3><p>Initiate a ollama server by following this tutorial <a href="https://github.com/ollama/ollama">here</a>. Then you can use it as follows:</p><pre class="with-bar"><code class="python -codedoc-code-snippet code-0-0-2" tabindex="0"><span class="wmbar-0-0-7"><span></span><span></span><span></span><span></span></span><div class="line-0-0-6  -codedoc-code-line" data-content="import from mtllm.llms.ollama, Ollama;" id="code8-l1"><span class="lineCounter-0-0-3 prim -codedoc-line-counter">1<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span><span class="token keyword">import</span><span class="token punctuation">:</span>py <span class="token keyword">from</span> mtllm<span class="token punctuation">.</span>llms<span class="token punctuation">.</span>ollama<span class="token punctuation">,</span> Ollama<span class="token punctuation">;</span></div><br><div class="line-0-0-6  -codedoc-code-line" data-content="" id="code8-l2"><span class="lineCounter-0-0-3 -codedoc-line-counter">2<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span></div><br><div class="line-0-0-6  -codedoc-code-line" data-content="llm = Ollama(host=&quot;ip:port of the ollama server&quot;, model_name=&quot;llama3&quot;, verbose=True/False);" id="code8-l3"><span class="lineCounter-0-0-3 prim -codedoc-line-counter">3<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span>llm <span class="token operator">=</span> Ollama<span class="token punctuation">(</span>host<span class="token operator">=</span><span class="token string">"ip:port of the ollama server"</span><span class="token punctuation">,</span> model_name<span class="token operator">=</span><span class="token string">"llama3"</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token operator">/</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">;</span></div><br></code></pre><h3 id="huggingface" class="heading-0-0-15"><span class="anchor-0-0-16" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>HuggingFace</h3><p>You can use any of the HuggingFace's language models as well. <a href="https://huggingface.co/models?pipeline_tag=text-generation">models</a></p><pre class="with-bar"><code class="python -codedoc-code-snippet code-0-0-2" tabindex="0"><span class="wmbar-0-0-7"><span></span><span></span><span></span><span></span></span><div class="line-0-0-6  -codedoc-code-line" data-content="import from mtllm.llms.huggingface, HuggingFace;" id="code9-l1"><span class="lineCounter-0-0-3 prim -codedoc-line-counter">1<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span><span class="token keyword">import</span><span class="token punctuation">:</span>py <span class="token keyword">from</span> mtllm<span class="token punctuation">.</span>llms<span class="token punctuation">.</span>huggingface<span class="token punctuation">,</span> HuggingFace<span class="token punctuation">;</span></div><br><div class="line-0-0-6  -codedoc-code-line" data-content="" id="code9-l2"><span class="lineCounter-0-0-3 -codedoc-line-counter">2<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span></div><br><div class="line-0-0-6  -codedoc-code-line" data-content="llm = HuggingFace(model_name=&quot;microsoft/Phi-3-mini-4k-instruct&quot;, verbose=True/False);" id="code9-l3"><span class="lineCounter-0-0-3 prim -codedoc-line-counter">3<span class="-codedoc-line-link"><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span></span>llm <span class="token operator">=</span> HuggingFace<span class="token punctuation">(</span>model_name<span class="token operator">=</span><span class="token string">"microsoft/Phi-3-mini-4k-instruct"</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token operator">/</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">;</span></div><br></code></pre><blockquote><p> <strong>NOTICE</strong></p><p>We are constantly adding new LMs to the library. If you want to add a new LM, please open an issue <a href="https://github.com/Jaseci-Labs/mtllm/issues">here</a>.</p></blockquote><div class="contentnav-0-0-14" data-no-search=""><a href="#language-models" class="h1" data-content-highlight="language-models">Language Models</a><a href="#setting-up-a-lm-client" class="h2" data-content-highlight="setting-up-a-lm-client">Setting up a LM client</a><a href="#directly-calling-the-lm" class="h2" data-content-highlight="directly-calling-the-lm">Directly calling the LM</a><a href="#using-the-lm-with-mtllm" class="h2" data-content-highlight="using-the-lm-with-mtllm">Using the LM with MTLLM</a><a href="#with-abilities-and-methods" class="h3" data-content-highlight="with-abilities-and-methods">With Abilities and Methods</a><a href="#with-classes" class="h3" data-content-highlight="with-classes">With Classes</a><a href="#you-can-parse-following-attributes-to-the-by-llm-feature" class="h3" data-content-highlight="you-can-parse-following-attributes-to-the-by-llm-feature">You can parse following attributes to the by llm() feature:</a><a href="#enabling-verbose-mode" class="h2" data-content-highlight="enabling-verbose-mode">Enabling Verbose Mode</a><a href="#remote-lms" class="h2" data-content-highlight="remote-lms">Remote LMs</a><a href="#local-lms" class="h2" data-content-highlight="local-lms">Local LMs</a><a href="#ollama" class="h3" data-content-highlight="ollama">Ollama</a><a href="#huggingface" class="h3" data-content-highlight="huggingface">HuggingFace</a></div></div><div id="-codedoc-toc" class="toc-0-0-11"><script>
     if (window.matchMedia('(min-width: 1200px)').matches) {
       if (!localStorage.getItem('-codedoc-toc-active')) {
         localStorage.setItem('-codedoc-toc-active', "true");
       }
     }
     </script><div class="content-0-0-12"><p><a href="/mtllm/">Home</a></p><div class="collapse-0-0-8 "><script id="RFlBgtVnev">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("RFlBgtVnev", "BR5Z0MA6Aj4P2zER2ZLlUg==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script><div class="label" onclick="this.parentElement.classList.toggle('open')"><span class="text">Quick Start</span><span class="icon-font closed">chevron_right</span></div><div class="content"><p><a href="/mtllm/docs/quickstart/installation">Installation</a>
<a href="/mtllm/docs/quickstart/minimal-working-example">Minimal Working Example</a></p></div></div><div class="collapse-0-0-8 "><script id="KnQqQPqoGf">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("KnQqQPqoGf", "BR5Z0MA6Aj4P2zER2ZLlUg==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script><div class="label" onclick="this.parentElement.classList.toggle('open')"><span class="text">Design and Implementation</span><span class="icon-font closed">chevron_right</span></div><div class="content"><p><a href="/mtllm/docs/design-impl/sem_registry">SemRegistry</a>
<a href="/mtllm/docs/design-impl/inference_engine">Inference Engine</a></p></div></div><div class="collapse-0-0-8 "><script id="RBOHckHBvN">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("RBOHckHBvN", "BR5Z0MA6Aj4P2zER2ZLlUg==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script><div class="label" onclick="this.parentElement.classList.toggle('open')"><span class="text">Building Blocks</span><span class="icon-font closed">chevron_right</span></div><div class="content"><p><a href="/mtllm/docs/building-blocks/language_models">Language Models</a>
<a href="/mtllm/docs/building-blocks/semstrings">Semstrings</a>
<a href="/mtllm/docs/building-blocks/functions_methods">Functions and Methods</a>
<a href="/mtllm/docs/building-blocks/object_init">Object Initialization</a>
<a href="/mtllm/docs/building-blocks/multimodality">Multimodality</a></p></div></div><div class="collapse-0-0-8 "><script id="hnKXZyNejT">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("hnKXZyNejT", "BR5Z0MA6Aj4P2zER2ZLlUg==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script><div class="label" onclick="this.parentElement.classList.toggle('open')"><span class="text">Tutorials</span><span class="icon-font closed">chevron_right</span></div><div class="content"><p><a href="/mtllm/docs/tutorials/rpg_game">RPG Game Level Generation</a></p></div></div><div class="collapse-0-0-8 "><script id="wJHQEJrbat">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("wJHQEJrbat", "BR5Z0MA6Aj4P2zER2ZLlUg==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script><div class="label" onclick="this.parentElement.classList.toggle('open')"><span class="text">API Reference</span><span class="icon-font closed">chevron_right</span></div><div class="content"><p><a href="/mtllm/docs/api/mtllm">MTLLM</a></p></div></div><div class="collapse-0-0-8 "><script id="qdBnApBq_w">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("qdBnApBq_w", "BR5Z0MA6Aj4P2zER2ZLlUg==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script><div class="label" onclick="this.parentElement.classList.toggle('open')"><span class="text">Tips and Tricks</span><span class="icon-font closed">chevron_right</span></div><div class="content"><p><a href="/mtllm/docs/tips-n-tricks/existing_application">Using MTLLM in your existing Application</a>
<a href="/mtllm/docs/tips-n-tricks/when_to_use_semstrings">When to use Semstrings</a>
<a href="/mtllm/docs/tips-n-tricks/create_own_lm">Create your own Language Model</a></p></div></div><p><a href="/mtllm/docs/faqs">FAQs</a></p></div><div class="search-0-0-13"><script id="nfpCYfkCdX">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("nfpCYfkCdX", "CEp7LAl0nnWrqHIN8Qnt6g==", {"repo":"mtllm","user":"Jaseci-Labs","root":"docs/md","pick":"\\.md$","drop":"(^_)|(\\/_)"});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script></div></div><div class="footer-0-0-10"><div class="left"><script id="PotlVvJOG_">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("PotlVvJOG_", "KKHOIeoEcuIIR8G+qI09PQ==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script></div><div class="main"><div class="inside"><a href="https://github.com/Jaseci-Labs/mtllm/" target="_blank">GitHub</a></div></div><div class="right"><script id="eBAcsVGhxh">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("eBAcsVGhxh", "Xodqq8f8LP13F67p+cusew==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script></div></div><script id="wjkfITiBRL">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("wjkfITiBRL", "3GUK3xGbIE9fCSzaoTX0bA==", {"namespace":"/mtllm"});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); window.setImmediate = window.setImmediate || function(f){setTimeout(f, 0)}; })()</script></body></html>