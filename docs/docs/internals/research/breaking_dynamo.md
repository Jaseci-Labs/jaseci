# ðŸ§  TorchDynamo FX Graphs: A Deep Dive
> _Exploring the internals of FX graph tracing â€” and the places it goes south._

Pytorch 2.0 introduced a novel framework for lowering pytorch models into accelarator hardware. The main two components of this framework is Torch Dynamo (frontend compiler) and Torch Inductor (backend compiler). On a high level, the torch dynamo genarates an fx-graph from python bytecode, which is an intermediat representation of code than can be compiled for accelarator hardware which is the compiled using the TorchInducter to produce Triton code.

In this document we explore the TorchDynamo frontend compiler and how it genarate fx-graph IR from bytecode. Some of the main research questions that we strive top answer from this exploration are,

1. How torch Dynamo decide what to compile?
2. Where graph breaks are applied in Dynamo?
3. Are each of these graph breaks always necessary?

## Into the Deep End

We will now go though the pipline of genarating the fx-graph through TorchDynamo using the (pytorch codebase)[https://github.com/pytorch/pytorch] and many other resorces.

The compilation process starts with ```torch.compile``` call for a defined function. In the following example we show this in action.

```python linenums="1"
class Model(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        y = torch.sin(x)
        z = torch.cos(x)

        # Combine and return
        return y**2 + z**2

model = Model()
compiled = torch.compile(model)
```

When this compiled model is called with input values it will produce fx-graph IR for the foward function. The fx graph is can be defined as dataflow in a forward function for op-to-op. This flow help the backend compiler to identify paralaizable operaations and schedule the operations accordingly. The genarated fx-graph looks like below.

![FX Graph Example](assets/main_count(1)_77.svg)

How ever, this graph genaration can be interrupted by adding an unsupported operation such as a python ```print``` statement. Here the goal would be to identify where exactly such breaks can occur.

## How torch Dynamo decide what to compile?

When tracing the codebase from the ```torch.compile``` one interesting find was the way how torch dynamo decides which function to compile. This is importent when an object gets passed into ```torch.compile```, Dynamo need to decide which methods needs to be compiled. When ```torch.compile``` is called, it fetches the python stack frames and use a set of criterion on which frame needs compiling. Following are the criteria for skiping compilation on a given stack.

1. If the frame is already compiled.
2. When debugging a specific function.
3. Skips generator expressions in specific utility files to reduce noise in error stats.
4. Avoids the complexity of compiling attribute setters (```__setattr__```), which can be tricky to handle generally.
5. Skips intitialization methods (```__init__```).
6. Avoids frames created by ```exec()``` calls, which can propagate globals unexpectedly.
7. Skips namedtuple subclass constructors that have empty builtins.
8. Generators cannot be compiled directly and will raise an error.
9. Skips frames that don't contain tensor operations.

These skips can be observed in the pytorch codebase.


=== "torch/_dynamo/convert_frame.py"
    ```python linenums="1"
    class ConvertFrameAssert:
        def __call__ (...):
            code = frame.f_code

            if code in output_codes:
                return ConvertFrameReturn()
            if (
                os.environ.get("TORCHDYNAMO_DEBUG_FUNCTION")
                and os.environ.get("TORCHDYNAMO_DEBUG_FUNCTION") != code.co_name
            ):
                return ConvertFrameReturn()
            if code.co_name == "<genexpr>" and code.co_filename.endswith(
                (
                    "transformers/file_utils.py",
                    "transformers/utils/generic.py",
                    "diffusers/utils/outputs.py",
                )
            ):
                # not needed, but cleans up torchbench error stats
                return ConvertFrameReturn()
            if code.co_name == "__setattr__":
                # setattr could be tricky to handle generally,
                # but also not likely useful to compile- skip the whole frame
                return ConvertFrameReturn()
            if code.co_name == "__init__" and code.co_filename.startswith(
                os.path.dirname(torch.optim.__file__)
            ):
                # optimizer support is still incomplete see
                # test_state_dict in test/dynamo/test_optimizers.py
                return ConvertFrameReturn()

            # Check if the frame is generated by an exec builtin call
            # TODO - Running exec generated frame seems propagates f_globals to the
            # next frames.
            if code.co_name == "<module>" and code.co_filename == "<string>":
                return ConvertFrameReturn()

            if (
                code.co_name == "<lambda>"
                and code.co_filename == "<string>"
                and not bool(frame.f_builtins)
            ):
                # namedtuple subclass constructor. Empty builtins cause issue with
                # len keyword in LIST_LEN guard.
                return ConvertFrameReturn()

            if is_generator(code):
                unimplemented_v2(
                    gb_type="Attempt to trace generator",
                    context="",
                    explanation="Generators cannot be compiled directly with `torch.compile`.",
                    hints=[
                        "Call a generator from inside of a non-generator Python function and "
                        "compile that function instead.",
                        *graph_break_hints.FUNDAMENTAL,
                    ],
                )

            if not has_tensor_in_frame(frame):
                return ConvertFrameReturn()

    ```

