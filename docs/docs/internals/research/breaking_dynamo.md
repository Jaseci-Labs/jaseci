# ðŸ”¥ TorchDynamo FX Graphs: A Deep Dive
> _Exploring the internals of FX graph tracing â€” and the places it goes south._

PyTorch 2.0 introduced a novel framework for lowering PyTorch models into accelerator hardware. The main two components of this framework are TorchDynamo (frontend compiler) and TorchInductor (backend compiler). On a high level, TorchDynamo generates an FX-graph from Python bytecode, which is an intermediate representation of code that can be compiled for accelerator hardware, which is then compiled using TorchInductor to produce Triton code.

In this document, we explore the TorchDynamo frontend compiler and how it generates FX-graph IR from bytecode. Some of the main research questions that we strive to answer from this exploration are:

1. How does TorchDynamo decide what to compile?
2. Where are graph breaks applied in Dynamo?
3. Are each of these graph breaks always necessary?

## Into the Deep End

We will now go through the pipeline of generating the FX-graph through TorchDynamo using the [PyTorch codebase](https://github.com/pytorch/pytorch) and many other resources.

The compilation process starts with a `torch.compile` call for a defined function. In the following example, we show this in action.

```python linenums="1"
class Model(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        y = torch.sin(x)
        z = torch.cos(x)

        # Combine and return
        return y**2 + z**2

model = Model()
compiled = torch.compile(model)
```

When this compiled model is called with input values, it will produce FX-graph IR for the forward function. The FX graph can be defined as dataflow in a forward function for op-to-op. This flow helps the backend compiler to identify parallelizable operations and schedule the operations accordingly. The generated FX-graph looks like below.

![FX Graph Example](assets/main_count(1)_77.svg)

However, this graph generation can be interrupted by adding an unsupported operation such as a Python `print` statement. Here, the goal would be to identify where exactly such breaks can occur.

## How does TorchDynamo decide what to compile?

When tracing the codebase from the `torch.compile` call, one interesting finding was the way TorchDynamo decides which function to compile. This is important when an object gets passed into `torch.compile`; Dynamo needs to decide which methods need to be compiled. When `torch.compile` is called, it fetches the Python stack frames and uses a set of criteria to determine which frame needs compiling. Following are the criteria for skipping compilation on a given stack:

1. If the frame is already compiled.
2. When debugging a specific function.
3. Skips generator expressions in specific utility files to reduce noise in error stats.
4. Avoids the complexity of compiling attribute setters (`__setattr__`), which can be tricky to handle generally.
5. Skips initialization methods (`__init__`).
6. Avoids frames created by `exec()` calls, which can propagate globals unexpectedly.
7. Skips namedtuple subclass constructors that have empty builtins.
8. Generators cannot be compiled directly and will raise an error.
9. Skips frames that don't contain tensor operations.

These skips can be observed in the PyTorch codebase.


=== "torch/_dynamo/convert_frame.py"
    ```python linenums="1"
    class ConvertFrameAssert:
        def __call__ (...):
            code = frame.f_code

            if code in output_codes:
                return ConvertFrameReturn()
            if (
                os.environ.get("TORCHDYNAMO_DEBUG_FUNCTION")
                and os.environ.get("TORCHDYNAMO_DEBUG_FUNCTION") != code.co_name
            ):
                return ConvertFrameReturn()
            if code.co_name == "<genexpr>" and code.co_filename.endswith(
                (
                    "transformers/file_utils.py",
                    "transformers/utils/generic.py",
                    "diffusers/utils/outputs.py",
                )
            ):
                # not needed, but cleans up torchbench error stats
                return ConvertFrameReturn()
            if code.co_name == "__setattr__":
                # setattr could be tricky to handle generally,
                # but also not likely useful to compile- skip the whole frame
                return ConvertFrameReturn()
            if code.co_name == "__init__" and code.co_filename.startswith(
                os.path.dirname(torch.optim.__file__)
            ):
                # optimizer support is still incomplete see
                # test_state_dict in test/dynamo/test_optimizers.py
                return ConvertFrameReturn()

            # Check if the frame is generated by an exec builtin call
            # TODO - Running exec generated frame seems propagates f_globals to the
            # next frames.
            if code.co_name == "<module>" and code.co_filename == "<string>":
                return ConvertFrameReturn()

            if (
                code.co_name == "<lambda>"
                and code.co_filename == "<string>"
                and not bool(frame.f_builtins)
            ):
                # namedtuple subclass constructor. Empty builtins cause issue with
                # len keyword in LIST_LEN guard.
                return ConvertFrameReturn()

            if is_generator(code):
                unimplemented_v2(
                    gb_type="Attempt to trace generator",
                    context="",
                    explanation="Generators cannot be compiled directly with `torch.compile`.",
                    hints=[
                        "Call a generator from inside of a non-generator Python function and "
                        "compile that function instead.",
                        *graph_break_hints.FUNDAMENTAL,
                    ],
                )

            if not has_tensor_in_frame(frame):
                return ConvertFrameReturn()

    ```
The skips are signified in this code as prematured return statements in the `__call__` method where the last method call is a the compilation method for which the frame object is passed into.
