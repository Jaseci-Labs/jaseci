{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonalizedHeader(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        None\n",
    "    def forward(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = torch.nn.TransformerEncoderLayer(\n",
    "                d_model=128,\n",
    "                nhead=2,\n",
    "                batch_first=True  # (batch, seq, feature)\n",
    ")\n",
    "encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=128, out_features=2, bias=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = torch.nn.Linear(128,2)\n",
    "torch.nn.init.xavier_uniform_(decoder.weight)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.ph_nlayer = ph_nlayer\n",
    "        if self.ph_nlayer != 0:\n",
    "            encoder_layer = torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.document_embeddings.embedding_length,\n",
    "                nhead=ph_nhead,\n",
    "                dim_feedforward=ph_ff_dim,\n",
    "                batch_first=True  # (batch, seq, feature)\n",
    "            )\n",
    "            self.encoder = torch.nn.TransformerEncoder(  # PH\n",
    "                encoder_layer=encoder_layer, num_layers=self.ph_nlayer\n",
    "            )\n",
    "        # ========================== End of added ==========================\n",
    "        self.decoder = nn.Linear(\n",
    "            self.document_embeddings.embedding_length, len(self.label_dictionary)\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.decoder.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0226, -0.0429, -0.0170,  0.0651,  0.0658,  0.0559, -0.0639, -0.0009,\n",
       "         -0.0022,  0.0699, -0.0592, -0.0342, -0.0740,  0.0643,  0.0132,  0.0104,\n",
       "         -0.0580, -0.0620,  0.0454,  0.0649, -0.0470,  0.0368,  0.0189, -0.0742,\n",
       "         -0.0712,  0.0559, -0.0139,  0.0066,  0.0098,  0.0344, -0.0513, -0.0333,\n",
       "          0.0235, -0.0560, -0.0657, -0.0377,  0.0792,  0.0829,  0.0785, -0.0250,\n",
       "          0.0465, -0.0175,  0.0394, -0.0179,  0.0461, -0.0074, -0.0207,  0.0439,\n",
       "          0.0054, -0.0045, -0.0163,  0.0565,  0.0150, -0.0312,  0.0803,  0.0335,\n",
       "         -0.0408,  0.0610, -0.0069,  0.0806,  0.0633, -0.0542,  0.0655,  0.0578,\n",
       "         -0.0295, -0.0415, -0.0716,  0.0424,  0.0326, -0.0605, -0.0304,  0.0030,\n",
       "         -0.0347,  0.0353,  0.0869,  0.0347,  0.0731, -0.0323, -0.0612, -0.0162,\n",
       "         -0.0249, -0.0122, -0.0393, -0.0568,  0.0396,  0.0555,  0.0263, -0.0684,\n",
       "          0.0338, -0.0365,  0.0796, -0.0237,  0.0338, -0.0773,  0.0494, -0.0495,\n",
       "          0.0408, -0.0412,  0.0044,  0.0333, -0.0277, -0.0031,  0.0042,  0.0157,\n",
       "         -0.0439, -0.0538, -0.0450,  0.0792,  0.0285,  0.0277, -0.0407,  0.0818,\n",
       "         -0.0591, -0.0456, -0.0398,  0.0524,  0.0046, -0.0466,  0.0199, -0.0457,\n",
       "          0.0239,  0.0590, -0.0580,  0.0776, -0.0149,  0.0701, -0.0587,  0.0624],\n",
       "        [-0.0670,  0.0205,  0.0333,  0.0689, -0.0882, -0.0758, -0.0629,  0.0194,\n",
       "         -0.0300,  0.0137,  0.0063, -0.0161,  0.0781, -0.0470,  0.0771, -0.0721,\n",
       "         -0.0390,  0.0430, -0.0105, -0.0359,  0.0355,  0.0550,  0.0658, -0.0664,\n",
       "         -0.0843, -0.0140,  0.0716, -0.0226,  0.0659, -0.0188, -0.0474,  0.0560,\n",
       "          0.0508, -0.0838, -0.0161,  0.0449,  0.0146, -0.0051, -0.0777, -0.0710,\n",
       "         -0.0783,  0.0348, -0.0792,  0.0009, -0.0527,  0.0502,  0.0343,  0.0327,\n",
       "         -0.0260,  0.0357, -0.0069, -0.0826,  0.0240,  0.0017, -0.0225, -0.0721,\n",
       "         -0.0462, -0.0315,  0.0603, -0.0710,  0.0098, -0.0853, -0.0383,  0.0146,\n",
       "          0.0529, -0.0535, -0.0546,  0.0328,  0.0197, -0.0849, -0.0152, -0.0585,\n",
       "         -0.0688, -0.0342,  0.0512,  0.0232, -0.0049, -0.0440, -0.0775,  0.0714,\n",
       "          0.0368,  0.0591, -0.0787, -0.0111,  0.0449,  0.0722, -0.0720,  0.0684,\n",
       "          0.0068,  0.0778,  0.0381,  0.0244, -0.0223,  0.0371,  0.0640,  0.0720,\n",
       "          0.0833, -0.0679,  0.0483,  0.0054,  0.0633,  0.0400,  0.0736,  0.0769,\n",
       "          0.0755, -0.0073, -0.0449,  0.0004,  0.0036, -0.0633,  0.0747,  0.0049,\n",
       "         -0.0353,  0.0468, -0.0165,  0.0545, -0.0002,  0.0547,  0.0127, -0.0874,\n",
       "          0.0829,  0.0646, -0.0561, -0.0607,  0.0116, -0.0719, -0.0083, -0.0624]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.nn.Linear(128,2)\n",
    "w.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0077, -0.0156,  0.1681,  0.0460,  0.2123, -0.2078, -0.0763, -0.1112,\n",
       "          0.1093,  0.1003,  0.0824,  0.0184,  0.1423, -0.1141, -0.1800, -0.1295,\n",
       "          0.1281, -0.0980,  0.0702, -0.1653,  0.1444,  0.2000,  0.2093,  0.2058,\n",
       "          0.0653,  0.1098, -0.1302, -0.1421, -0.1827,  0.0327, -0.1585,  0.1335,\n",
       "         -0.1708, -0.0970,  0.0465,  0.0930,  0.0414, -0.1750, -0.1063,  0.1252,\n",
       "          0.2119, -0.1761,  0.1983, -0.1755,  0.0326,  0.2005,  0.0711,  0.1611,\n",
       "          0.0033, -0.0720,  0.1642, -0.2020,  0.1379,  0.0553,  0.1461, -0.1897,\n",
       "         -0.0751,  0.1253,  0.1791, -0.0710,  0.0280,  0.1373,  0.1241, -0.1883,\n",
       "          0.1794, -0.1867,  0.1495, -0.1789,  0.0321,  0.0461,  0.0673, -0.0002,\n",
       "          0.1890,  0.1473,  0.0355, -0.0845,  0.2003, -0.0744,  0.0403,  0.1301,\n",
       "         -0.1253,  0.1624, -0.1385, -0.1405, -0.1624, -0.2048,  0.0509,  0.0489,\n",
       "          0.0623,  0.1236,  0.1778, -0.1464,  0.0765,  0.1070,  0.1557,  0.1937,\n",
       "          0.0860, -0.0582, -0.0489, -0.1755, -0.0720,  0.1655,  0.1409,  0.1825,\n",
       "         -0.0686,  0.1605,  0.0659,  0.1118,  0.0433, -0.0434,  0.1156,  0.2071,\n",
       "          0.0099, -0.0392,  0.0158,  0.0422, -0.2049, -0.0351, -0.0697, -0.1978,\n",
       "          0.1404,  0.1065,  0.0652,  0.1968, -0.1955, -0.0963,  0.0715, -0.0340],\n",
       "        [-0.0879,  0.0178, -0.0752,  0.1526,  0.0838,  0.1626, -0.1439, -0.1979,\n",
       "          0.0425, -0.0844,  0.1352, -0.1601, -0.0034,  0.1467, -0.0493,  0.1341,\n",
       "          0.0798, -0.0236, -0.0361, -0.1988, -0.0445,  0.0396,  0.0742,  0.0059,\n",
       "         -0.0133, -0.0643,  0.0414,  0.1165,  0.1410, -0.0098,  0.0627, -0.0662,\n",
       "          0.0526, -0.1263,  0.1519,  0.1677, -0.2004, -0.1613, -0.1199,  0.1200,\n",
       "         -0.1677,  0.0349, -0.1932, -0.1074,  0.0335, -0.1679,  0.0197,  0.1477,\n",
       "          0.1917,  0.0515, -0.1487,  0.0025, -0.0407,  0.1804,  0.0833,  0.1190,\n",
       "         -0.1771,  0.1127,  0.0979, -0.1054,  0.1504,  0.1597, -0.2056, -0.1323,\n",
       "          0.0078,  0.1582,  0.0733, -0.0057,  0.1042,  0.0735, -0.1262,  0.2107,\n",
       "          0.1205,  0.1141, -0.0848, -0.2038,  0.2031, -0.0635,  0.0938,  0.1795,\n",
       "          0.1274, -0.0169, -0.0780, -0.0101, -0.2029,  0.0641,  0.0802,  0.0731,\n",
       "         -0.1176,  0.0028, -0.0680,  0.1879,  0.0434,  0.0384,  0.0884, -0.1032,\n",
       "         -0.1084, -0.0061, -0.0233,  0.1478,  0.1318,  0.1756, -0.0255, -0.0950,\n",
       "         -0.1297, -0.1253,  0.1821,  0.0273,  0.1303,  0.1230, -0.1893,  0.0466,\n",
       "         -0.1911,  0.0750,  0.1810,  0.2101, -0.0926,  0.0318,  0.1774, -0.1729,\n",
       "          0.1174,  0.1560,  0.1345,  0.1285,  0.0631, -0.0132,  0.1408,  0.1069]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(w.weight)\n",
    "w.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b6787d877d891d6f35d3f7884c8abb5e5f96961ddd42559d74a40e85b1bf5aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
