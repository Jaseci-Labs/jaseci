[TRAIN_PARAMETERS]
max_contexts_length = 128
max_candidate_length = 64
train_batch_size = 8
eval_batch_size = 1
max_history = 4
learning_rate = 0.00005
weight_decay = 0.1
warmup_steps = 2000
adam_epsilon = 0.00000008
max_grad_norm = 1
num_train_epochs = 5
seed = 12345
gradient_accumulation_steps = 1
fp16 = False
fp16_opt_level = O1
gpu = 0
basepath = modeloutput

[MODEL_PARAMETERS]
shared = False
model_name = distilbert-base-uncased

